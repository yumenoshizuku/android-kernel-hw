diff --git a/include/linux/sched.h b/include/linux/sched.h
index ff6bb0f..44f078a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -39,6 +39,7 @@
 #define SCHED_BATCH		3
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
+#define SCHED_MYCFS		6
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
 
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 3ede7d9..ebac653 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -11,7 +11,7 @@ ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER),y)
 CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
-obj-y += core.o clock.o idle_task.o fair.o rt.o stop_task.o sched_avg.o
+obj-y += core.o clock.o idle_task.o fair.o rt.o stop_task.o sched_avg.o mycfs.o
 obj-$(CONFIG_SMP) += cpupri.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += auto_group.o
 obj-$(CONFIG_SCHEDSTATS) += stats.o
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 1cee48f..3123239 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -4107,7 +4107,7 @@ recheck:
 
 		if (policy != SCHED_FIFO && policy != SCHED_RR &&
 				policy != SCHED_NORMAL && policy != SCHED_BATCH &&
-				policy != SCHED_IDLE)
+				policy != SCHED_MYCFS && policy != SCHED_IDLE)
 			return -EINVAL;
 	}
 
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index fc60d5b..d240ab2 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5536,7 +5536,7 @@ static unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task
  * All the scheduling class methods:
  */
 const struct sched_class fair_sched_class = {
-	.next			= &idle_sched_class,
+	.next			= &mycfs_sched_class,
 	.enqueue_task		= enqueue_task_fair,
 	.dequeue_task		= dequeue_task_fair,
 	.yield_task		= yield_task_fair,
diff --git a/kernel/sched/mycfs.c b/kernel/sched/mycfs.c
new file mode 100644
index 0000000..98de2df
--- /dev/null
+++ b/kernel/sched/mycfs.c
@@ -0,0 +1,1453 @@
+#include <linux/latencytop.h>
+#include <linux/sched.h>
+#include <linux/cpumask.h>
+#include <linux/slab.h>
+#include <linux/profile.h>
+#include <linux/interrupt.h>
+
+#include <trace/events/sched.h>
+
+#include "sched.h"
+
+/*
+ * Targeted preemption latency for CPU-bound tasks:
+ * (default: 6ms * (1 + ilog(ncpus)), units: nanoseconds)
+ *
+ * NOTE: this latency value is not the same as the concept of
+ * 'timeslice length' - timeslices in CFS are of variable length
+ * and have no persistent notion like in traditional, time-slice
+ * based scheduling concepts.
+ *
+ * (to see the precise effective timeslice length of your workload,
+ *  run vmstat and monitor the context-switches (cs) field)
+ */
+unsigned int sysctl_sched_latency_mycfs = 10000000ULL;
+unsigned int normalized_sysctl_sched_latency_mycfs = 10000000ULL;
+
+/*
+ * The initial- and re-scaling of tunables is configurable
+ * (default SCHED_TUNABLESCALING_LOG = *(1+ilog(ncpus))
+ *
+ * Options are:
+ * SCHED_TUNABLESCALING_NONE - unscaled, always *1
+ * SCHED_TUNABLESCALING_LOG - scaled logarithmical, *1+ilog(ncpus)
+ * SCHED_TUNABLESCALING_LINEAR - scaled linear, *ncpus
+ */
+enum sched_tunable_scaling sysctl_sched_tunable_scaling_mycfs
+	= SCHED_TUNABLESCALING_LOG;
+
+/*
+ * Minimal preemption granularity for CPU-bound tasks:
+ * (default: 0.75 msec * (1 + ilog(ncpus)), units: nanoseconds)
+ */
+unsigned int sysctl_sched_min_granularity_mycfs = 750000ULL;
+unsigned int normalized_sysctl_sched_min_granularity_mycfs = 750000ULL;
+
+/*
+ * is kept at sysctl_sched_latency_mycfs / sysctl_sched_min_granularity_mycfs
+ */
+static unsigned int sched_nr_latency_mycfs = 8;
+
+/*
+ * After fork, child runs first. If set to 0 (default) then
+ * parent will (try to) run first.
+ */
+unsigned int sysctl_sched_child_runs_first_mycfs __read_mostly;
+
+/*
+ * SCHED_OTHER wake-up granularity.
+ * (default: 1 msec * (1 + ilog(ncpus)), units: nanoseconds)
+ *
+ * This option delays the preemption effects of decoupled workloads
+ * and reduces their over-scheduling. Synchronous workloads will still
+ * have immediate wakeup/sleep latencies.
+ */
+unsigned int sysctl_sched_wakeup_granularity_mycfs = 1000000UL;
+unsigned int normalized_sysctl_sched_wakeup_granularity_mycfs = 1000000UL;
+
+/*
+ * Increase the granularity value when there are more CPUs,
+ * because with more CPUs the 'effective latency' as visible
+ * to users decreases. But the relationship is not linear,
+ * so pick a second-best guess by going with the log2 of the
+ * number of CPUs.
+ *
+ * This idea comes from the SD scheduler of Con Kolivas:
+ */
+static int get_update_sysctl_factor_mycfs(void)
+{
+	unsigned int cpus = min_t(int, num_online_cpus(), 8);
+	unsigned int factor;
+
+	switch (sysctl_sched_tunable_scaling_mycfs) {
+	case SCHED_TUNABLESCALING_NONE:
+		factor = 1;
+		break;
+	case SCHED_TUNABLESCALING_LINEAR:
+		factor = cpus;
+		break;
+	case SCHED_TUNABLESCALING_LOG:
+	default:
+		factor = 1 + ilog2(cpus);
+		break;
+	}
+
+	return factor;
+}
+
+static void update_sysctl_mycfs(void)
+{
+	unsigned int factor = get_update_sysctl_factor_mycfs();
+	sysctl_sched_min_granularity_mycfs = factor * normalized_sysctl_sched_min_granularity_mycfs;
+	sysctl_sched_latency_mycfs = factor * normalized_sysctl_sched_latency_mycfs;
+	sysctl_sched_wakeup_granularity_mycfs = factor * normalized_sysctl_sched_wakeup_granularity_mycfs;
+
+}
+
+void sched_init_granularity_mycfs(void)
+{
+	update_sysctl_mycfs();
+}
+
+#if BITS_PER_LONG == 32
+# define WMULT_CONST	(~0UL)
+#else
+# define WMULT_CONST	(1UL << 32)
+#endif
+
+#define WMULT_SHIFT	32
+
+/*
+ * Shift right and round:
+ */
+#define SRR(x, y) (((x) + (1UL << ((y) - 1))) >> (y))
+
+/*
+ * delta *= weight / lw
+ */
+static unsigned long
+calc_delta_mine_mycfs(unsigned long delta_exec, unsigned long weight,
+		struct load_weight *lw)
+{
+	u64 tmp;
+
+	/*
+	 * weight can be less than 2^SCHED_LOAD_RESOLUTION for task group sched
+	 * entities since MIN_SHARES = 2. Treat weight as 1 if less than
+	 * 2^SCHED_LOAD_RESOLUTION.
+	 */
+	if (likely(weight > (1UL << SCHED_LOAD_RESOLUTION)))
+		tmp = (u64)delta_exec * scale_load_down(weight);
+	else
+		tmp = (u64)delta_exec;
+
+	if (!lw->inv_weight) {
+		unsigned long w = scale_load_down(lw->weight);
+
+		if (BITS_PER_LONG > 32 && unlikely(w >= WMULT_CONST))
+			lw->inv_weight = 1;
+		else if (unlikely(!w))
+			lw->inv_weight = WMULT_CONST;
+		else
+			lw->inv_weight = WMULT_CONST / w;
+	}
+
+	/*
+	 * Check whether we'd overflow the 64-bit multiplication:
+	 */
+	if (unlikely(tmp > WMULT_CONST))
+		tmp = SRR(SRR(tmp, WMULT_SHIFT/2) * lw->inv_weight,
+			WMULT_SHIFT/2);
+	else
+		tmp = SRR(tmp * lw->inv_weight, WMULT_SHIFT);
+
+	return (unsigned long)min(tmp, (u64)(unsigned long)LONG_MAX);
+}
+
+
+const struct sched_class mycfs_sched_class;
+
+/**************************************************************
+ * CFS operations on generic schedulable entities:
+ */
+
+
+static inline struct task_struct *task_of_mycfs(struct sched_entity *se)
+{
+	return container_of(se, struct task_struct, se);
+}
+
+static inline struct rq *rq_of_mycfs(struct mycfs_rq *mycfs_rq)
+{
+	return container_of(mycfs_rq, struct rq, mycfs);
+}
+
+#define entity_is_task(se)	1
+
+#define for_each_sched_entity(se) \
+		for (; se; se = NULL)
+
+static inline struct mycfs_rq *task_mycfs_rq(struct task_struct *p)
+{
+	return &task_rq(p)->mycfs;
+}
+
+static inline struct mycfs_rq *mycfs_rq_of(struct sched_entity *se)
+{
+	struct task_struct *p = task_of_mycfs(se);
+	struct rq *rq = task_rq(p);
+
+	return &rq->mycfs;
+}
+
+/* runqueue "owned" by this group */
+static inline struct mycfs_rq *group_mycfs_rq(struct sched_entity *grp)
+{
+	return NULL;
+}
+
+static inline void list_add_leaf_mycfs_rq(struct mycfs_rq *mycfs_rq)
+{
+}
+
+static inline void list_del_leaf_mycfs_rq(struct mycfs_rq *mycfs_rq)
+{
+}
+
+#define for_each_leaf_mycfs_rq(rq, mycfs_rq) \
+		for (mycfs_rq = &rq->mycfs; mycfs_rq; mycfs_rq = NULL)
+
+static inline struct sched_entity *parent_entity_mycfs(struct sched_entity *se)
+{
+	return NULL;
+}
+
+static inline void
+find_matching_se_mycfs(struct sched_entity **se, struct sched_entity **pse)
+{
+}
+
+
+static __always_inline
+void account_mycfs_rq_runtime(struct mycfs_rq *mycfs_rq, unsigned long delta_exec);
+
+/**************************************************************
+ * Scheduling class tree data structure manipulation methods:
+ */
+
+static inline u64 max_vruntime_mycfs(u64 min_vruntime, u64 vruntime)
+{
+	s64 delta = (s64)(vruntime - min_vruntime);
+	if (delta > 0)
+		min_vruntime = vruntime;
+
+	return min_vruntime;
+}
+
+static inline u64 min_vruntime_mycfs(u64 min_vruntime, u64 vruntime)
+{
+	s64 delta = (s64)(vruntime - min_vruntime);
+	if (delta < 0)
+		min_vruntime = vruntime;
+
+	return min_vruntime;
+}
+
+static inline int entity_before_mycfs(struct sched_entity *a,
+				struct sched_entity *b)
+{
+	return (s64)(a->vruntime - b->vruntime) < 0;
+}
+
+static void update_min_vruntime_mycfs(struct mycfs_rq *mycfs_rq)
+{
+	u64 vruntime = mycfs_rq->min_vruntime;
+
+	if (mycfs_rq->curr)
+		vruntime = mycfs_rq->curr->vruntime;
+
+	if (mycfs_rq->rb_leftmost) {
+		struct sched_entity *se = rb_entry(mycfs_rq->rb_leftmost,
+						   struct sched_entity,
+						   run_node);
+
+		if (!mycfs_rq->curr)
+			vruntime = se->vruntime;
+		else
+			vruntime = min_vruntime_mycfs(vruntime, se->vruntime);
+	}
+
+	mycfs_rq->min_vruntime = max_vruntime_mycfs(mycfs_rq->min_vruntime, vruntime);
+#ifndef CONFIG_64BIT
+	smp_wmb();
+	mycfs_rq->min_vruntime_copy = mycfs_rq->min_vruntime;
+#endif
+}
+
+/*
+ * Enqueue an entity into the rb-tree:
+ */
+static void __enqueue_entity_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	struct rb_node **link = &mycfs_rq->tasks_timeline.rb_node;
+	struct rb_node *parent = NULL;
+	struct sched_entity *entry;
+	int leftmost = 1;
+
+	/*
+	 * Find the right place in the rbtree:
+	 */
+	while (*link) {
+		parent = *link;
+		entry = rb_entry(parent, struct sched_entity, run_node);
+		/*
+		 * We dont care about collisions. Nodes with
+		 * the same key stay together.
+		 */
+		if (entity_before_mycfs(se, entry)) {
+			link = &parent->rb_left;
+		} else {
+			link = &parent->rb_right;
+			leftmost = 0;
+		}
+	}
+
+	/*
+	 * Maintain a cache of leftmost tree entries (it is frequently
+	 * used):
+	 */
+	if (leftmost)
+		mycfs_rq->rb_leftmost = &se->run_node;
+
+	rb_link_node(&se->run_node, parent, link);
+	rb_insert_color(&se->run_node, &mycfs_rq->tasks_timeline);
+}
+
+static void __dequeue_entity_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	if (mycfs_rq->rb_leftmost == &se->run_node) {
+		struct rb_node *next_node;
+
+		next_node = rb_next(&se->run_node);
+		mycfs_rq->rb_leftmost = next_node;
+	}
+
+	rb_erase(&se->run_node, &mycfs_rq->tasks_timeline);
+}
+
+struct sched_entity *__pick_first_entity_mycfs(struct mycfs_rq *mycfs_rq)
+{
+	struct rb_node *left = mycfs_rq->rb_leftmost;
+
+	if (!left)
+		return NULL;
+
+	return rb_entry(left, struct sched_entity, run_node);
+}
+
+static struct sched_entity *__pick_next_entity_mycfs(struct sched_entity *se)
+{
+	struct rb_node *next = rb_next(&se->run_node);
+
+	if (!next)
+		return NULL;
+
+	return rb_entry(next, struct sched_entity, run_node);
+}
+
+/*
+ * delta /= w
+ */
+static inline unsigned long
+calc_delta_mycfs(unsigned long delta, struct sched_entity *se)
+{
+	if (unlikely(se->load.weight != NICE_0_LOAD))
+		delta = calc_delta_mine_mycfs(delta, NICE_0_LOAD, &se->load);
+
+	return delta;
+}
+
+/*
+ * The idea is to set a period in which each task runs once.
+ *
+ * When there are too many tasks (sysctl_sched_nr_latency_mycfs) we have to stretch
+ * this period because otherwise the slices get too small.
+ *
+ * p = (nr <= nl) ? l : l*nr/nl
+ */
+static u64 __sched_period_mycfs(unsigned long nr_running)
+{
+	u64 period = sysctl_sched_latency_mycfs;
+	unsigned long nr_latency = sched_nr_latency_mycfs;
+
+	if (unlikely(nr_running > nr_latency)) {
+		period = sysctl_sched_min_granularity_mycfs;
+		period *= nr_running;
+	}
+
+	return period;
+}
+
+/*
+ * We calculate the wall-time slice from the period by taking a part
+ * proportional to the weight.
+ *
+ * s = p*P[w/rw]
+ */
+static u64 sched_slice_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	u64 slice = __sched_period_mycfs(mycfs_rq->nr_running + !se->on_rq);
+
+	for_each_sched_entity(se) {
+		struct load_weight *load;
+		struct load_weight lw;
+
+		mycfs_rq = mycfs_rq_of(se);
+		load = &mycfs_rq->load;
+
+		if (unlikely(!se->on_rq)) {
+			lw = mycfs_rq->load;
+
+			update_load_add(&lw, se->load.weight);
+			load = &lw;
+		}
+		slice = calc_delta_mine_mycfs(slice, se->load.weight, load);
+	}
+	return slice;
+}
+
+/*
+ * We calculate the vruntime slice of a to be inserted task
+ *
+ * vs = s/w
+ */
+static u64 sched_vslice_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	return calc_delta_mycfs(sched_slice_mycfs(mycfs_rq, se), se);
+}
+
+static void update_mycfs_load(struct mycfs_rq *mycfs_rq, int global_update);
+static void update_mycfs_shares(struct mycfs_rq *mycfs_rq);
+
+/*
+ * Update the current task's runtime statistics. Skip current tasks that
+ * are not in our scheduling class.
+ */
+static inline void
+__update_curr_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *curr,
+	      unsigned long delta_exec)
+{
+	unsigned long delta_exec_weighted;
+
+	schedstat_set(curr->statistics.exec_max,
+		      max((u64)delta_exec, curr->statistics.exec_max));
+
+	curr->sum_exec_runtime += delta_exec;
+	schedstat_add(mycfs_rq, exec_clock, delta_exec);
+	delta_exec_weighted = calc_delta_mycfs(delta_exec, curr);
+
+	curr->vruntime += delta_exec_weighted;
+	update_min_vruntime_mycfs(mycfs_rq);
+}
+
+static void update_curr_mycfs(struct mycfs_rq *mycfs_rq)
+{
+	struct sched_entity *curr = mycfs_rq->curr;
+	u64 now = rq_of_mycfs(mycfs_rq)->clock_task;
+	unsigned long delta_exec;
+
+	if (unlikely(!curr))
+		return;
+
+	/*
+	 * Get the amount of time the current task was running
+	 * since the last time we changed load (this cannot
+	 * overflow on 32 bits):
+	 */
+	delta_exec = (unsigned long)(now - curr->exec_start);
+	if (!delta_exec)
+		return;
+
+	__update_curr_mycfs(mycfs_rq, curr, delta_exec);
+	curr->exec_start = now;
+
+	if (entity_is_task(curr)) {
+		struct task_struct *curtask = task_of_mycfs(curr);
+
+		trace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);
+		cpuacct_charge(curtask, delta_exec);
+		account_group_exec_runtime(curtask, delta_exec);
+	}
+
+	account_mycfs_rq_runtime(mycfs_rq, delta_exec);
+}
+
+static inline void
+update_stats_wait_start_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	schedstat_set(se->statistics.wait_start, rq_of_mycfs(mycfs_rq)->clock);
+}
+
+/*
+ * Task is being enqueued - update stats:
+ */
+static void update_stats_enqueue_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	/*
+	 * Are we enqueueing a waiting task? (for current tasks
+	 * a dequeue/enqueue event is a NOP)
+	 */
+	if (se != mycfs_rq->curr)
+		update_stats_wait_start_mycfs(mycfs_rq, se);
+}
+
+static void
+update_stats_wait_end_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	schedstat_set(se->statistics.wait_max, max(se->statistics.wait_max,
+			rq_of_mycfs(mycfs_rq)->clock - se->statistics.wait_start));
+	schedstat_set(se->statistics.wait_count, se->statistics.wait_count + 1);
+	schedstat_set(se->statistics.wait_sum, se->statistics.wait_sum +
+			rq_of_mycfs(mycfs_rq)->clock - se->statistics.wait_start);
+	schedstat_set(se->statistics.wait_start, 0);
+}
+
+static inline void
+update_stats_dequeue_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	/*
+	 * Mark the end of the wait period if dequeueing a
+	 * waiting task:
+	 */
+	if (se != mycfs_rq->curr)
+		update_stats_wait_end_mycfs(mycfs_rq, se);
+}
+
+/*
+ * We are picking a new current task - update its stats:
+ */
+static inline void
+update_stats_curr_start_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	/*
+	 * We are starting a new run period:
+	 */
+	se->exec_start = rq_of_mycfs(mycfs_rq)->clock_task;
+}
+
+/**************************************************
+ * Scheduling class queueing methods:
+ */
+
+static void
+account_entity_enqueue_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	update_load_add(&mycfs_rq->load, se->load.weight);
+	if (!parent_entity_mycfs(se))
+		update_load_add(&rq_of_mycfs(mycfs_rq)->load, se->load.weight);
+	mycfs_rq->nr_running++;
+}
+
+static void
+account_entity_dequeue_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	update_load_sub(&mycfs_rq->load, se->load.weight);
+	if (!parent_entity_mycfs(se))
+		update_load_sub(&rq_of_mycfs(mycfs_rq)->load, se->load.weight);
+	if (entity_is_task(se))
+		list_del_init(&se->group_node);
+	mycfs_rq->nr_running--;
+}
+
+static void update_mycfs_load(struct mycfs_rq *mycfs_rq, int global_update)
+{
+}
+
+static inline void update_mycfs_shares(struct mycfs_rq *mycfs_rq)
+{
+}
+
+static inline void update_entity_shares_tick_mycfs(struct mycfs_rq *mycfs_rq)
+{
+}
+
+static void enqueue_sleeper_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+}
+
+static void check_spread_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+}
+
+static void
+place_entity_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se, int initial)
+{
+	u64 vruntime = mycfs_rq->min_vruntime;
+
+	/*
+	 * The 'current' period is already promised to the current tasks,
+	 * however the extra weight of the new task will slow them down a
+	 * little, place the new task so that it fits in the slot that
+	 * stays open at the end.
+	 */
+	if (initial && sched_feat(START_DEBIT))
+		vruntime += sched_vslice_mycfs(mycfs_rq, se);
+
+	/* sleeps up to a single latency don't count. */
+	if (!initial) {
+		unsigned long thresh = sysctl_sched_latency_mycfs;
+
+		/*
+		 * Halve their sleep time's effect, to allow
+		 * for a gentler effect of sleepers:
+		 */
+		if (sched_feat(GENTLE_FAIR_SLEEPERS))
+			thresh >>= 1;
+
+		vruntime -= thresh;
+	}
+
+	/* ensure we never gain time by being placed backwards. */
+	vruntime = max_vruntime_mycfs(se->vruntime, vruntime);
+
+	se->vruntime = vruntime;
+}
+
+static void check_enqueue_throttle_mycfs(struct mycfs_rq *mycfs_rq);
+
+static void
+enqueue_entity_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se, int flags)
+{
+	/*
+	 * Update the normalized vruntime before updating min_vruntime
+	 * through callig update_curr_mycfs().
+	 */
+	if (!(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_WAKING))
+		se->vruntime += mycfs_rq->min_vruntime;
+
+	/*
+	 * Update run-time statistics of the 'current'.
+	 */
+	update_curr_mycfs(mycfs_rq);
+	update_mycfs_load(mycfs_rq, 0);
+	account_entity_enqueue_mycfs(mycfs_rq, se);
+	update_mycfs_shares(mycfs_rq);
+
+	if (flags & ENQUEUE_WAKEUP) {
+		place_entity_mycfs(mycfs_rq, se, 0);
+		enqueue_sleeper_mycfs(mycfs_rq, se);
+	}
+
+	update_stats_enqueue_mycfs(mycfs_rq, se);
+	check_spread_mycfs(mycfs_rq, se);
+	if (se != mycfs_rq->curr)
+		__enqueue_entity_mycfs(mycfs_rq, se);
+	se->on_rq = 1;
+
+	if (mycfs_rq->nr_running == 1) {
+		list_add_leaf_mycfs_rq(mycfs_rq);
+		check_enqueue_throttle_mycfs(mycfs_rq);
+	}
+}
+
+static void __clear_buddies_last_mycfs(struct sched_entity *se)
+{
+	for_each_sched_entity(se) {
+		struct mycfs_rq *mycfs_rq = mycfs_rq_of(se);
+		if (mycfs_rq->last == se)
+			mycfs_rq->last = NULL;
+		else
+			break;
+	}
+}
+
+static void __clear_buddies_next_mycfs(struct sched_entity *se)
+{
+	for_each_sched_entity(se) {
+		struct mycfs_rq *mycfs_rq = mycfs_rq_of(se);
+		if (mycfs_rq->next == se)
+			mycfs_rq->next = NULL;
+		else
+			break;
+	}
+}
+
+static void __clear_buddies_skip_mycfs(struct sched_entity *se)
+{
+	for_each_sched_entity(se) {
+		struct mycfs_rq *mycfs_rq = mycfs_rq_of(se);
+		if (mycfs_rq->skip == se)
+			mycfs_rq->skip = NULL;
+		else
+			break;
+	}
+}
+
+static void clear_buddies_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	if (mycfs_rq->last == se)
+		__clear_buddies_last_mycfs(se);
+
+	if (mycfs_rq->next == se)
+		__clear_buddies_next_mycfs(se);
+
+	if (mycfs_rq->skip == se)
+		__clear_buddies_skip_mycfs(se);
+}
+
+static __always_inline void return_mycfs_rq_runtime(struct mycfs_rq *mycfs_rq);
+
+static void
+dequeue_entity_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se, int flags)
+{
+	/*
+	 * Update run-time statistics of the 'current'.
+	 */
+	update_curr_mycfs(mycfs_rq);
+
+	update_stats_dequeue_mycfs(mycfs_rq, se);
+	if (flags & DEQUEUE_SLEEP) {
+	}
+
+	clear_buddies_mycfs(mycfs_rq, se);
+
+	if (se != mycfs_rq->curr)
+		__dequeue_entity_mycfs(mycfs_rq, se);
+	se->on_rq = 0;
+	update_mycfs_load(mycfs_rq, 0);
+	account_entity_dequeue_mycfs(mycfs_rq, se);
+
+	/*
+	 * Normalize the entity after updating the min_vruntime because the
+	 * update can refer to the ->curr item and we need to reflect this
+	 * movement in our normalized position.
+	 */
+	if (!(flags & DEQUEUE_SLEEP))
+		se->vruntime -= mycfs_rq->min_vruntime;
+
+	/* return excess runtime on last dequeue */
+	return_mycfs_rq_runtime(mycfs_rq);
+
+	update_min_vruntime_mycfs(mycfs_rq);
+	update_mycfs_shares(mycfs_rq);
+}
+
+/*
+ * Preempt the current task with a newly woken task if needed:
+ */
+static void
+check_preempt_tick_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *curr)
+{
+	unsigned long ideal_runtime, delta_exec;
+	struct sched_entity *se;
+	s64 delta;
+
+	ideal_runtime = sched_slice_mycfs(mycfs_rq, curr);
+	delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;
+	if (delta_exec > ideal_runtime) {
+		resched_task(rq_of_mycfs(mycfs_rq)->curr);
+		/*
+		 * The current task ran long enough, ensure it doesn't get
+		 * re-elected due to buddy favours.
+		 */
+		clear_buddies_mycfs(mycfs_rq, curr);
+		return;
+	}
+
+	/*
+	 * Ensure that a task that missed wakeup preemption by a
+	 * narrow margin doesn't have to wait for a full slice.
+	 * This also mitigates buddy induced latencies under load.
+	 */
+	if (delta_exec < sysctl_sched_min_granularity_mycfs)
+		return;
+
+	se = __pick_first_entity_mycfs(mycfs_rq);
+	delta = curr->vruntime - se->vruntime;
+
+	if (delta < 0)
+		return;
+
+	if (delta > ideal_runtime)
+		resched_task(rq_of_mycfs(mycfs_rq)->curr);
+}
+
+static void
+set_next_entity_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *se)
+{
+	/* 'current' is not kept within the tree. */
+	if (se->on_rq) {
+		/*
+		 * Any task has to be enqueued before it get to execute on
+		 * a CPU. So account for the time it spent waiting on the
+		 * runqueue.
+		 */
+		update_stats_wait_end_mycfs(mycfs_rq, se);
+		__dequeue_entity_mycfs(mycfs_rq, se);
+	}
+
+	update_stats_curr_start_mycfs(mycfs_rq, se);
+	mycfs_rq->curr = se;
+	se->prev_sum_exec_runtime = se->sum_exec_runtime;
+}
+
+static int
+wakeup_preempt_entity_mycfs(struct sched_entity *curr, struct sched_entity *se);
+
+/*
+ * Pick the next process, keeping these things in mind, in this order:
+ * 1) keep things fair between processes/task groups
+ * 2) pick the "next" process, since someone really wants that to run
+ * 3) pick the "last" process, for cache locality
+ * 4) do not run the "skip" process, if something else is available
+ */
+static struct sched_entity *pick_next_entity_mycfs(struct mycfs_rq *mycfs_rq)
+{
+	struct sched_entity *se = __pick_first_entity_mycfs(mycfs_rq);
+	struct sched_entity *left = se;
+
+	/*
+	 * Avoid running the skip buddy, if running something else can
+	 * be done without getting too unfair.
+	 */
+	if (mycfs_rq->skip == se) {
+		struct sched_entity *second = __pick_next_entity_mycfs(se);
+		if (second && wakeup_preempt_entity_mycfs(second, left) < 1)
+			se = second;
+	}
+
+	/*
+	 * Prefer last buddy, try to return the CPU to a preempted task.
+	 */
+	if (mycfs_rq->last && wakeup_preempt_entity_mycfs(mycfs_rq->last, left) < 1)
+		se = mycfs_rq->last;
+
+	/*
+	 * Someone really wants this to run. If it's not unfair, run it.
+	 */
+	if (mycfs_rq->next && wakeup_preempt_entity_mycfs(mycfs_rq->next, left) < 1)
+		se = mycfs_rq->next;
+
+	clear_buddies_mycfs(mycfs_rq, se);
+
+	return se;
+}
+
+static void check_mycfs_rq_runtime(struct mycfs_rq *mycfs_rq);
+
+static void put_prev_entity_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *prev)
+{
+	/*
+	 * If still on the runqueue then deactivate_task()
+	 * was not called and update_curr_mycfs() has to be done:
+	 */
+	if (prev->on_rq)
+		update_curr_mycfs(mycfs_rq);
+
+	/* throttle cfs_rqs exceeding runtime */
+	check_mycfs_rq_runtime(mycfs_rq);
+
+	check_spread_mycfs(mycfs_rq, prev);
+	if (prev->on_rq) {
+		update_stats_wait_start_mycfs(mycfs_rq, prev);
+		/* Put 'current' back into the tree. */
+		__enqueue_entity_mycfs(mycfs_rq, prev);
+	}
+	mycfs_rq->curr = NULL;
+}
+
+static void
+entity_tick_mycfs(struct mycfs_rq *mycfs_rq, struct sched_entity *curr, int queued)
+{
+	/*
+	 * Update run-time statistics of the 'current'.
+	 */
+	update_curr_mycfs(mycfs_rq);
+
+	/*
+	 * Update share accounting for long-running entities.
+	 */
+	update_entity_shares_tick_mycfs(mycfs_rq);
+
+	if (mycfs_rq->nr_running > 1)
+		check_preempt_tick_mycfs(mycfs_rq, curr);
+}
+
+
+static __always_inline
+void account_mycfs_rq_runtime(struct mycfs_rq *mycfs_rq, unsigned long delta_exec) {}
+static void check_mycfs_rq_runtime(struct mycfs_rq *mycfs_rq) {}
+static void check_enqueue_throttle_mycfs(struct mycfs_rq *mycfs_rq) {}
+static __always_inline void return_mycfs_rq_runtime(struct mycfs_rq *mycfs_rq) {}
+
+static inline int mycfs_rq_throttled(struct mycfs_rq *mycfs_rq)
+{
+	return 0;
+}
+
+static inline int throttled_hierarchy_mycfs(struct mycfs_rq *mycfs_rq)
+{
+	return 0;
+}
+
+static inline int throttled_lb_pair_mycfs(struct task_group *tg,
+				    int src_cpu, int dest_cpu)
+{
+	return 0;
+}
+
+static inline void
+hrtick_start_mycfs(struct rq *rq, struct task_struct *p)
+{
+}
+
+static inline void hrtick_update_mycfs(struct rq *rq)
+{
+}
+
+/*
+ * The enqueue_task method is called before nr_running is
+ * increased. Here we update the fair scheduling stats and
+ * then put the task into the rbtree:
+ */
+static void
+enqueue_task_mycfs(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct mycfs_rq *mycfs_rq;
+	struct sched_entity *se = &p->se;
+
+	for_each_sched_entity(se) {
+		if (se->on_rq)
+			break;
+		mycfs_rq = mycfs_rq_of(se);
+		enqueue_entity_mycfs(mycfs_rq, se, flags);
+
+		/*
+		 * end evaluation on encountering a throttled cfs_rq
+		 *
+		 * note: in the case of encountering a throttled cfs_rq we will
+		 * post the final h_nr_running increment below.
+		*/
+		if (mycfs_rq_throttled(mycfs_rq))
+			break;
+		mycfs_rq->h_nr_running++;
+
+		flags = ENQUEUE_WAKEUP;
+	}
+
+	for_each_sched_entity(se) {
+		mycfs_rq = mycfs_rq_of(se);
+		mycfs_rq->h_nr_running++;
+
+		if (mycfs_rq_throttled(mycfs_rq))
+			break;
+
+		update_mycfs_load(mycfs_rq, 0);
+		update_mycfs_shares(mycfs_rq);
+	}
+
+	if (!se)
+		inc_nr_running(rq);
+	hrtick_update_mycfs(rq);
+}
+
+static void set_next_buddy_mycfs(struct sched_entity *se);
+
+/*
+ * The dequeue_task method is called before nr_running is
+ * decreased. We remove the task from the rbtree and
+ * update the fair scheduling stats:
+ */
+static void dequeue_task_mycfs(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct mycfs_rq *mycfs_rq;
+	struct sched_entity *se = &p->se;
+	int task_sleep = flags & DEQUEUE_SLEEP;
+
+	for_each_sched_entity(se) {
+		mycfs_rq = mycfs_rq_of(se);
+		dequeue_entity_mycfs(mycfs_rq, se, flags);
+
+		/*
+		 * end evaluation on encountering a throttled cfs_rq
+		 *
+		 * note: in the case of encountering a throttled cfs_rq we will
+		 * post the final h_nr_running decrement below.
+		*/
+		if (mycfs_rq_throttled(mycfs_rq))
+			break;
+		mycfs_rq->h_nr_running--;
+
+		/* Don't dequeue parent if it has other entities besides us */
+		if (mycfs_rq->load.weight) {
+			/*
+			 * Bias pick_next to pick a task from this cfs_rq, as
+			 * p is sleeping when it is within its sched_slice_mycfs.
+			 */
+			if (task_sleep && parent_entity_mycfs(se))
+				set_next_buddy_mycfs(parent_entity_mycfs(se));
+
+			/* avoid re-evaluating load for this entity */
+			se = parent_entity_mycfs(se);
+			break;
+		}
+		flags |= DEQUEUE_SLEEP;
+	}
+
+	for_each_sched_entity(se) {
+		mycfs_rq = mycfs_rq_of(se);
+		mycfs_rq->h_nr_running--;
+
+		if (mycfs_rq_throttled(mycfs_rq))
+			break;
+
+		update_mycfs_load(mycfs_rq, 0);
+		update_mycfs_shares(mycfs_rq);
+	}
+
+	if (!se)
+		dec_nr_running(rq);
+	hrtick_update_mycfs(rq);
+}
+
+static unsigned long
+wakeup_gran(struct sched_entity *curr, struct sched_entity *se)
+{
+	unsigned long gran = sysctl_sched_wakeup_granularity_mycfs;
+
+	/*
+	 * Since its curr running now, convert the gran from real-time
+	 * to virtual-time in his units.
+	 *
+	 * By using 'se' instead of 'curr' we penalize light tasks, so
+	 * they get preempted easier. That is, if 'se' < 'curr' then
+	 * the resulting gran will be larger, therefore penalizing the
+	 * lighter, if otoh 'se' > 'curr' then the resulting gran will
+	 * be smaller, again penalizing the lighter task.
+	 *
+	 * This is especially important for buddies when the leftmost
+	 * task is higher priority than the buddy.
+	 */
+	return calc_delta_mycfs(gran, se);
+}
+
+/*
+ * Should 'se' preempt 'curr'.
+ *
+ *             |s1
+ *        |s2
+ *   |s3
+ *         g
+ *      |<--->|c
+ *
+ *  w(c, s1) = -1
+ *  w(c, s2) =  0
+ *  w(c, s3) =  1
+ *
+ */
+static int
+wakeup_preempt_entity_mycfs(struct sched_entity *curr, struct sched_entity *se)
+{
+	s64 gran, vdiff = curr->vruntime - se->vruntime;
+
+	if (vdiff <= 0)
+		return -1;
+
+	gran = wakeup_gran(curr, se);
+	if (vdiff > gran)
+		return 1;
+
+	return 0;
+}
+
+static void set_last_buddy(struct sched_entity *se)
+{
+	if (entity_is_task(se) && unlikely(task_of_mycfs(se)->policy == SCHED_IDLE))
+		return;
+
+	for_each_sched_entity(se)
+		mycfs_rq_of(se)->last = se;
+}
+
+static void set_next_buddy_mycfs(struct sched_entity *se)
+{
+	if (entity_is_task(se) && unlikely(task_of_mycfs(se)->policy == SCHED_IDLE))
+		return;
+
+	for_each_sched_entity(se)
+		mycfs_rq_of(se)->next = se;
+}
+
+static void set_skip_buddy_mycfs(struct sched_entity *se)
+{
+	for_each_sched_entity(se)
+		mycfs_rq_of(se)->skip = se;
+}
+
+/*
+ * Preempt the current task with a newly woken task if needed:
+ */
+static void check_preempt_wakeup_mycfs(struct rq *rq, struct task_struct *p, int wake_flags)
+{
+	struct task_struct *curr = rq->curr;
+	struct sched_entity *se = &curr->se, *pse = &p->se;
+	struct mycfs_rq *mycfs_rq = task_mycfs_rq(curr);
+	int scale = mycfs_rq->nr_running >= sched_nr_latency_mycfs;
+	int next_buddy_marked = 0;
+
+	if (unlikely(se == pse))
+		return;
+
+	/*
+	 * This is possible from callers such as move_task(), in which we
+	 * unconditionally check_prempt_curr() after an enqueue (which may have
+	 * lead to a throttle).  This both saves work and prevents false
+	 * next-buddy nomination below.
+	 */
+	if (unlikely(throttled_hierarchy_mycfs(mycfs_rq_of(pse))))
+		return;
+
+	if (sched_feat(NEXT_BUDDY) && scale && !(wake_flags & WF_FORK)) {
+		set_next_buddy_mycfs(pse);
+		next_buddy_marked = 1;
+	}
+
+	/*
+	 * We can come here with TIF_NEED_RESCHED already set from new task
+	 * wake up path.
+	 *
+	 * Note: this also catches the edge-case of curr being in a throttled
+	 * group (e.g. via set_curr_task), since update_curr_mycfs() (in the
+	 * enqueue of curr) will have resulted in resched being set.  This
+	 * prevents us from potentially nominating it as a false LAST_BUDDY
+	 * below.
+	 */
+	if (test_tsk_need_resched(curr))
+		return;
+
+	/* Idle tasks are by definition preempted by non-idle tasks. */
+	if (unlikely(curr->policy == SCHED_IDLE) &&
+	    likely(p->policy != SCHED_IDLE))
+		goto preempt;
+
+	/*
+	 * Batch and idle tasks do not preempt non-idle tasks (their preemption
+	 * is driven by the tick):
+	 */
+	if (unlikely(p->policy != SCHED_NORMAL))
+		return;
+
+	find_matching_se_mycfs(&se, &pse);
+	update_curr_mycfs(mycfs_rq_of(se));
+	BUG_ON(!pse);
+	if (wakeup_preempt_entity_mycfs(se, pse) == 1) {
+		/*
+		 * Bias pick_next to pick the sched entity that is
+		 * triggering this preemption.
+		 */
+		if (!next_buddy_marked)
+			set_next_buddy_mycfs(pse);
+		goto preempt;
+	}
+
+	return;
+
+preempt:
+	resched_task(curr);
+	/*
+	 * Only set the backward buddy when the current task is still
+	 * on the rq. This can happen when a wakeup gets interleaved
+	 * with schedule on the ->pre_schedule() or idle_balance()
+	 * point, either of which can * drop the rq lock.
+	 *
+	 * Also, during early boot the idle thread is in the fair class,
+	 * for obvious reasons its a bad idea to schedule back to it.
+	 */
+	if (unlikely(!se->on_rq || curr == rq->idle))
+		return;
+
+	if (sched_feat(LAST_BUDDY) && scale && entity_is_task(se))
+		set_last_buddy(se);
+}
+
+static struct task_struct *pick_next_task_mycfs(struct rq *rq)
+{
+	struct task_struct *p;
+	struct mycfs_rq *mycfs_rq = &rq->mycfs;
+	struct sched_entity *se;
+
+	if (!mycfs_rq->nr_running)
+		return NULL;
+
+	do {
+		se = pick_next_entity_mycfs(mycfs_rq);
+		set_next_entity_mycfs(mycfs_rq, se);
+		mycfs_rq = group_mycfs_rq(se);
+	} while (mycfs_rq);
+
+	p = task_of_mycfs(se);
+	if (hrtick_enabled(rq))
+		hrtick_start_mycfs(rq, p);
+
+	return p;
+}
+
+/*
+ * Account for a descheduled task:
+ */
+static void put_prev_task_mycfs(struct rq *rq, struct task_struct *prev)
+{
+	struct sched_entity *se = &prev->se;
+	struct mycfs_rq *mycfs_rq;
+
+	for_each_sched_entity(se) {
+		mycfs_rq = mycfs_rq_of(se);
+		put_prev_entity_mycfs(mycfs_rq, se);
+	}
+}
+
+/*
+ * sched_yield() is very simple
+ *
+ * The magic of dealing with the ->skip buddy is in pick_next_entity_mycfs.
+ */
+static void yield_task_mycfs(struct rq *rq)
+{
+	struct task_struct *curr = rq->curr;
+	struct mycfs_rq *mycfs_rq = task_mycfs_rq(curr);
+	struct sched_entity *se = &curr->se;
+
+	/*
+	 * Are we the only task in the tree?
+	 */
+	if (unlikely(rq->nr_running == 1))
+		return;
+
+	clear_buddies_mycfs(mycfs_rq, se);
+
+	if (curr->policy != SCHED_BATCH) {
+		update_rq_clock(rq);
+		/*
+		 * Update run-time statistics of the 'current'.
+		 */
+		update_curr_mycfs(mycfs_rq);
+		/*
+		 * Tell update_rq_clock() that we've just updated,
+		 * so we don't do microscopic update in schedule()
+		 * and double the fastpath cost.
+		 */
+		 rq->skip_clock_update = 1;
+	}
+
+	set_skip_buddy_mycfs(se);
+}
+
+static bool yield_to_task_mycfs(struct rq *rq, struct task_struct *p, bool preempt)
+{
+	struct sched_entity *se = &p->se;
+
+	/* throttled hierarchies are not runnable */
+	if (!se->on_rq || throttled_hierarchy_mycfs(mycfs_rq_of(se)))
+		return false;
+
+	/* Tell the scheduler that we'd really like pse to run next. */
+	set_next_buddy_mycfs(se);
+
+	yield_task_mycfs(rq);
+
+	return true;
+}
+
+/*
+ * scheduler tick hitting a task of our scheduling class:
+ */
+static void task_tick_mycfs(struct rq *rq, struct task_struct *curr, int queued)
+{
+	struct mycfs_rq *mycfs_rq;
+	struct sched_entity *se = &curr->se;
+
+	for_each_sched_entity(se) {
+		mycfs_rq = mycfs_rq_of(se);
+		entity_tick_mycfs(mycfs_rq, se, queued);
+	}
+}
+
+/*
+ * called on fork with the child task as argument from the parent's context
+ *  - child not yet on the tasklist
+ *  - preemption disabled
+ */
+static void task_fork_mycfs(struct task_struct *p)
+{
+	struct mycfs_rq *mycfs_rq;
+	struct sched_entity *se = &p->se, *curr;
+	int this_cpu = smp_processor_id();
+	struct rq *rq = this_rq();
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&rq->lock, flags);
+
+	update_rq_clock(rq);
+
+	mycfs_rq = task_mycfs_rq(current);
+	curr = mycfs_rq->curr;
+
+	if (unlikely(task_cpu(p) != this_cpu)) {
+		rcu_read_lock();
+		__set_task_cpu(p, this_cpu);
+		rcu_read_unlock();
+	}
+
+	update_curr_mycfs(mycfs_rq);
+
+	if (curr)
+		se->vruntime = curr->vruntime;
+	place_entity_mycfs(mycfs_rq, se, 1);
+
+	if (sysctl_sched_child_runs_first_mycfs && curr && entity_before_mycfs(curr, se)) {
+		/*
+		 * Upon rescheduling, sched_class::put_prev_task() will place
+		 * 'current' within the tree based on its new key value.
+		 */
+		swap(curr->vruntime, se->vruntime);
+		resched_task(rq->curr);
+	}
+
+	se->vruntime -= mycfs_rq->min_vruntime;
+
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+}
+
+/*
+ * Priority of the task has changed. Check to see if we preempt
+ * the current task.
+ */
+static void
+prio_changed_mycfs(struct rq *rq, struct task_struct *p, int oldprio)
+{
+	if (!p->se.on_rq)
+		return;
+
+	/*
+	 * Reschedule if we are currently running on this runqueue and
+	 * our priority decreased, or if we are not currently running on
+	 * this runqueue and our priority is higher than the current's
+	 */
+	if (rq->curr == p) {
+		if (p->prio > oldprio)
+			resched_task(rq->curr);
+	} else
+		check_preempt_curr(rq, p, 0);
+}
+
+static void switched_from_mycfs(struct rq *rq, struct task_struct *p)
+{
+	struct sched_entity *se = &p->se;
+	struct mycfs_rq *mycfs_rq = mycfs_rq_of(se);
+
+	/*
+	 * Ensure the task's vruntime is normalized, so that when its
+	 * switched back to the fair class the enqueue_entity_mycfs(.flags=0) will
+	 * do the right thing.
+	 *
+	 * If it was on_rq, then the dequeue_entity_mycfs(.flags=0) will already
+	 * have normalized the vruntime, if it was !on_rq, then only when
+	 * the task is sleeping will it still have non-normalized vruntime.
+	 */
+	if (!se->on_rq && p->state != TASK_RUNNING) {
+		/*
+		 * Fix up our vruntime so that the current sleep doesn't
+		 * cause 'unlimited' sleep bonus.
+		 */
+		place_entity_mycfs(mycfs_rq, se, 0);
+		se->vruntime -= mycfs_rq->min_vruntime;
+	}
+}
+
+/*
+ * We switched to the sched_fair class.
+ */
+static void switched_to_mycfs(struct rq *rq, struct task_struct *p)
+{
+	if (!p->se.on_rq)
+		return;
+
+	/*
+	 * We were most likely switched from sched_rt, so
+	 * kick off the schedule if running, otherwise just see
+	 * if we can still preempt the current task.
+	 */
+	if (rq->curr == p)
+		resched_task(rq->curr);
+	else
+		check_preempt_curr(rq, p, 0);
+}
+
+/* Account for a task changing its policy or group.
+ *
+ * This routine is mostly called to set cfs_rq->curr field when a task
+ * migrates between groups/classes.
+ */
+static void set_curr_task_mycfs(struct rq *rq)
+{
+	struct sched_entity *se = &rq->curr->se;
+
+	for_each_sched_entity(se) {
+		struct mycfs_rq *mycfs_rq = mycfs_rq_of(se);
+
+		set_next_entity_mycfs(mycfs_rq, se);
+		/* ensure bandwidth has been allocated on our new cfs_rq */
+		account_mycfs_rq_runtime(mycfs_rq, 0);
+	}
+}
+
+void init_mycfs_rq(struct mycfs_rq *mycfs_rq)
+{
+	mycfs_rq->tasks_timeline = RB_ROOT;
+	mycfs_rq->min_vruntime = (u64)(-(1LL << 20));
+#ifndef CONFIG_64BIT
+	mycfs_rq->min_vruntime_copy = mycfs_rq->min_vruntime;
+#endif
+}
+
+
+static unsigned int get_rr_interval_mycfs(struct rq *rq, struct task_struct *task)
+{
+	struct sched_entity *se = &task->se;
+	unsigned int rr_interval = 0;
+
+	/*
+	 * Time slice is 0 for SCHED_OTHER tasks that are on an otherwise
+	 * idle runqueue:
+	 */
+	if (rq->mycfs.load.weight)
+		rr_interval = NS_TO_JIFFIES(sched_slice_mycfs(&rq->mycfs, se));
+
+	return rr_interval;
+}
+
+/*
+ * All the scheduling class methods:
+ */
+const struct sched_class mycfs_sched_class = {
+	.next			= &idle_sched_class,
+	.enqueue_task		= enqueue_task_mycfs,
+	.dequeue_task		= dequeue_task_mycfs,
+	.yield_task		= yield_task_mycfs,
+	.yield_to_task		= yield_to_task_mycfs,
+
+	.check_preempt_curr	= check_preempt_wakeup_mycfs,
+
+	.pick_next_task		= pick_next_task_mycfs,
+	.put_prev_task		= put_prev_task_mycfs,
+
+	.set_curr_task          = set_curr_task_mycfs,
+	.task_tick		= task_tick_mycfs,
+	.task_fork		= task_fork_mycfs,
+
+	.prio_changed		= prio_changed_mycfs,
+	.switched_from		= switched_from_mycfs,
+	.switched_to		= switched_to_mycfs,
+
+	.get_rr_interval	= get_rr_interval_mycfs,
+};
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5370bcb..f1785b2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -273,6 +273,79 @@ struct cfs_rq {
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 };
 
+/* MYCFS */
+struct mycfs_rq {
+	struct load_weight load;
+	unsigned long nr_running, h_nr_running;
+
+	u64 exec_clock;
+	u64 min_vruntime;
+#ifndef CONFIG_64BIT
+	u64 min_vruntime_copy;
+#endif
+
+	struct rb_root tasks_timeline;
+	struct rb_node *rb_leftmost;
+
+	/*
+	 * 'curr' points to currently running entity on this cfs_rq.
+	 * It is set to NULL otherwise (i.e when none are currently running).
+	 */
+	struct sched_entity *curr, *next, *last, *skip;
+
+#ifdef	CONFIG_SCHED_DEBUG
+	unsigned int nr_spread_over;
+#endif
+
+#ifdef CONFIG_MYCFS_GROUP_SCHED
+	struct rq *rq;	/* cpu runqueue to which this cfs_rq is attached */
+
+	/*
+	 * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in
+	 * a hierarchy). Non-leaf lrqs hold other higher schedulable entities
+	 * (like users, containers etc.)
+	 *
+	 * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This
+	 * list is used during load balance.
+	 */
+	int on_list;
+	struct list_head leaf_cfs_rq_list;
+	struct task_group *tg;	/* group that "owns" this runqueue */
+
+#ifdef CONFIG_SMP
+	/*
+	 *   h_load = weight * f(tg)
+	 *
+	 * Where f(tg) is the recursive weight fraction assigned to
+	 * this group.
+	 */
+	unsigned long h_load;
+
+	/*
+	 * Maintaining per-cpu shares distribution for group scheduling
+	 *
+	 * load_stamp is the last time we updated the load average
+	 * load_last is the last time we updated the load average and saw load
+	 * load_unacc_exec_time is currently unaccounted execution time
+	 */
+	u64 load_avg;
+	u64 load_period;
+	u64 load_stamp, load_last, load_unacc_exec_time;
+
+	unsigned long load_contribution;
+#endif /* CONFIG_SMP */
+#ifdef CONFIG_CFS_BANDWIDTH
+	int runtime_enabled;
+	u64 runtime_expires;
+	s64 runtime_remaining;
+
+	u64 throttled_timestamp;
+	int throttled, throttle_count;
+	struct list_head throttled_list;
+#endif /* CONFIG_CFS_BANDWIDTH */
+#endif /* CONFIG_MYCFS_GROUP_SCHED */
+};
+
 static inline int rt_bandwidth_enabled(void)
 {
 	return sysctl_sched_rt_runtime >= 0;
@@ -371,12 +444,17 @@ struct rq {
 	u64 nr_switches;
 
 	struct cfs_rq cfs;
+	struct mycfs_rq mycfs;
 	struct rt_rq rt;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
 	struct list_head leaf_cfs_rq_list;
 #endif
+#ifdef CONFIG_MYCFS_GROUP_SCHED
+	/* list of leaf cfs_rq on this cpu: */
+	struct list_head leaf_mycfs_rq_list;
+#endif
 #ifdef CONFIG_RT_GROUP_SCHED
 	struct list_head leaf_rt_rq_list;
 #endif
@@ -856,6 +934,7 @@ enum cpuacct_stat_index {
 extern const struct sched_class stop_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
+extern const struct sched_class mycfs_sched_class;
 extern const struct sched_class idle_sched_class;
 
 
